{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidjosep/student-feedback-analysis/blob/master/Model_1_Actionable_Insights_from_Student_Feedback_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQyZX0C64pV"
      },
      "source": [
        "**Actionable Insights from Student Feedback: A Sentiment Analysis Approach**\n",
        "\n",
        "In the realm of education, student feedback is an invaluable resource for enhancing teaching methodologies and course structures. However, the traditional approach of relying on numerical ratings often fails to capture the nuanced sentiments and specific concerns of students. This project aims to revolutionize the way educators interpret and utilize student feedback by employing a sophisticated sentiment analysis approach.\n",
        "\n",
        "Our objective is to transform raw, textual student feedback into actionable insights, enabling educators to make informed decisions that align closely with student needs and preferences. The project involves three key components:\n",
        "\n",
        "1.   **Rating-Based Classification:** We begin by classifying student feedback into traditional rating categories (1-5). This provides a baseline understanding of student satisfaction and sets the stage for a deeper analysis.\n",
        "\n",
        "2.   **Nuanced Sentiment Analysis:** Moving beyond mere numerical ratings, we delve into the underlying sentiments of the feedback. Whether a student feels 'Frustrated', 'Depressed', or finds the course 'Boring', this model aims to identify and categorize these sentiments. This step is crucial as it helps in prioritizing issues based on their emotional impact, ensuring that more critical concerns like student mental well-being are addressed promptly.\n",
        "\n",
        "3.   **Aspect-Based Sentiment Analysis (ABSA):** The final and most intricate layer of our analysis focuses on specific aspects of the educational experience, such as 'Teaching Experience' and 'Lab Experience'. By dissecting feedback into these aspects and analyzing the associated sentiments, we provide educators with a detailed, aspect-wise breakdown of student opinions. This granular insight allows for targeted improvements in specific areas of the course.\n",
        "\n",
        "\n",
        "Utilizing a dataset of student feedback from Coursera, coupled with advanced machine learning models, this project sets out to automate the process of feedback analysis. This automation not only scales the feedback analysis process but also ensures a rapid and nuanced understanding of student sentiments. The anticipated outcome is a more effective, responsive, and student-centric educational strategy, ultimately fostering a supportive and conducive learning environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTWzr6K1V_OE",
        "outputId": "b4c609cc-c0ae-4384-bbf2-b0a18b2ccb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch 1/5\n",
            "1338/1338 [==============================] - 792s 587ms/step - loss: 0.6964 - accuracy: 0.7562 - val_loss: 0.6189 - val_accuracy: 0.7703\n",
            "Epoch 2/5\n",
            "1338/1338 [==============================] - 805s 602ms/step - loss: 0.5996 - accuracy: 0.7771 - val_loss: 0.5896 - val_accuracy: 0.7800\n",
            "Epoch 3/5\n",
            "1338/1338 [==============================] - 764s 571ms/step - loss: 0.5746 - accuracy: 0.7854 - val_loss: 0.5928 - val_accuracy: 0.7792\n",
            "Epoch 4/5\n",
            " 361/1338 [=======>......................] - ETA: 8:41 - loss: 0.5583 - accuracy: 0.7907"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stop words\n",
        "def remove_stop_words(text):\n",
        "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Teaching/TME_6015/Project/reviews.csv')\n",
        "\n",
        "# Apply stop word removal\n",
        "df['Review'] = df['Review'].apply(remove_stop_words)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
        "    df['Review'], df['Label'] - 1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the label encoder to your labels and transform them to integers\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=5000)  # Adjust num_words as needed\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_reviews)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_reviews)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=150)  # Adjust maxlen as needed\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=150)\n",
        "\n",
        "# Building the LSTM model with cuDNN optimization\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=150))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(128, dropout=0.2))  # Removed recurrent_dropout\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax', dtype='float32'))  # Ensure output layer uses float32\n",
        "\n",
        "\n",
        "# Compile the model with adjusted learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, train_labels, batch_size=64, epochs=5,\n",
        "                    validation_data=(X_test_pad, test_labels))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test_pad, test_labels)\n",
        "\n",
        "# Predict classes\n",
        "y_pred = model.predict(X_test_pad)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Labels')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(test_labels, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have the tokenizer used during training\n",
        "# tokenizer = ... (Load or create the tokenizer used during training)\n",
        "\n",
        "# Function to read JSON file\n",
        "def read_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Function to write JSON file\n",
        "def write_json(data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "\n",
        "def remove_stop_words_from_list(texts):\n",
        "    processed_texts = []\n",
        "    for text in texts:\n",
        "        processed_text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "        processed_texts.append(processed_text)\n",
        "    return processed_texts\n",
        "\n",
        "# Read test data\n",
        "test_data_dict = read_json('/content/drive/MyDrive/MCSC/TME_6015/Project/test_data.json')  # Make sure this path is correct\n",
        "\n",
        "# Extract messages from the test data\n",
        "test_messages = list(test_data_dict.values())\n",
        "test_messages = remove_stop_words_from_list(test_messages)\n",
        "# Preprocess test data\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_messages)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=150)  # Adjust maxlen if needed\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_pad)\n",
        "predicted_labels = label_encoder.inverse_transform([np.argmax(p) for p in predictions])\n",
        "\n",
        "# Prepare data for JSON output\n",
        "output_predictions = {id: str(label) for id, label in zip(test_data_dict.keys(), predicted_labels)}\n",
        "print(output_predictions)\n",
        "# Write predictions to JSON\n",
        "write_json(output_predictions, '/content/drive/MyDrive/MCSC/TME_6015/Project/ratings.json')\n",
        "\n",
        "print(\"Predictions saved to predictions.json\")\n"
      ],
      "metadata": {
        "id": "Cn1c__WqLmFL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}